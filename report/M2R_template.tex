\documentclass[a4paper,11pt, titlepage]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[a4paper,top=1in,bottom=1in,left=1in,right=1in,marginparwidth=1.75cm]{geometry}
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{listings}
\lstset{
    frame = single, 
    framexleftmargin=15pt
}
\usepackage{caption}
\captionsetup[figure]{labelfont={bf},name={Figure},labelsep=quad}
\captionsetup[table]{labelfont={bf},name={Table},labelsep=quad}
\usepackage[ruled,vlined]{algorithm2e}
\makeatletter
\renewcommand{\SetKwInOut}[2]{%
  \sbox\algocf@inoutbox{\KwSty{#2}\algocf@typo:}%
  \expandafter\ifx\csname InOutSizeDefined\endcsname\relax% if first time used
    \newcommand\InOutSizeDefined{}\setlength{\inoutsize}{\wd\algocf@inoutbox}%
    \sbox\algocf@inoutbox{\parbox[t]{\inoutsize}{\KwSty{#2}\algocf@typo:\hfill}~}\setlength{\inoutindent}{\wd\algocf@inoutbox}%
  \else% else keep the larger dimension
    \ifdim\wd\algocf@inoutbox>\inoutsize%
    \setlength{\inoutsize}{\wd\algocf@inoutbox}%
    \sbox\algocf@inoutbox{\parbox[t]{\inoutsize}{\KwSty{#2}\algocf@typo:\hfill}~}\setlength{\inoutindent}{\wd\algocf@inoutbox}%
    \fi%
  \fi% the dimension of the box is now defined.
  \algocf@newcommand{#1}[1]{%
    \ifthenelse{\boolean{algocf@inoutnumbered}}{\relax}{\everypar={\relax}}%
%     {\let\\\algocf@newinout\hangindent=\wd\algocf@inoutbox\hangafter=1\parbox[t]{\inoutsize}{\KwSty{#2}\algocf@typo\hfill:}~##1\par}%
    {\let\\\algocf@newinout\hangindent=\inoutindent\hangafter=1\parbox[t]{\inoutsize}{\KwSty{#2}\algocf@typo:\hfill}~##1\par}%
    \algocf@linesnumbered% reset the numbering of the lines
  }}%
\makeatother
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\renewcommand*{\rmdefault}{bch}
\renewcommand*{\ttdefault}{lmtt}
\newcommand{\citationneeded}{\textcolor{red}{[citation-needed]}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\setlength{\parskip}{0.5em}
\usepackage[numbers, comma, square, sort&compress]{natbib}
\bibliographystyle{abbrvunsrtnat.bst}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\reporttitle}{Scientific Machine Learning: Neural Ordinary and Control Differential Equations}
\newcommand{\reportauthorA}{James Tay (CID: 02015786)}
\newcommand{\reportauthorB}{Jiaru (Eric) Li (CID: 02216531)}
\newcommand{\reportauthorC}{Student name 3 (CID: -------------)}
\newcommand{\reportauthorD}{Student name 4 (CID: -------------)}
\newcommand{\reportauthorE}{Student name 5 (CID: -------------)}
\newcommand{\supervisor}{Sheehan Olver}

\begin{document}
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\includegraphics[width=8cm]{Imperial_logo.png}\\[1cm]
\center
\textsc{\LARGE Imperial College London}\\[0.5cm] 
\textsc{\Large Department of Mathematics}\\[1.5cm] 
\textsc{\Large Second-year Group Research Project}\\[0.5cm]
\makeatletter
\HRule \\[0.6cm]
{\huge \bfseries \reporttitle}\\[0.6cm]
\HRule \\[1.5cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
\reportauthorA \\
\reportauthorB \\
\reportauthorC \\
\reportauthorD \\
\reportauthorE
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor(s):} \\
\supervisor
\end{flushright}
\end{minipage}\\[2cm]
\makeatother
\vfill
\makeatletter
{\large \today}\\[2cm]
\makeatother
\end{titlepage}

\begin{abstract}
\textcolor{red}{Type your abstract here. The abstract is a summary of the contents of the project. It should be brief but informative, and
should avoid technicalities as far as possible.}
\end{abstract}
\tableofcontents
\clearpage

\section{Introduction}
\label{sec:introduction}

\textcolor{red}{The introduction should attempt to set your work in the context of other work done in the field. It
should demonstrate that you are aware of what you are doing, and how it relates to other work
(with references). It should also provide an overview of the contents of the project. You should
highlight your individual contributions and any novel result: which of the calculations, theorems,
examples, proofs, conjectures, codes etc. are your own?}

\section{Neural Networks}

\subsection{History and Introduction}

Generally speaking, a \textit{neural network} is a group of units called \textit{neurons} connected by signals called \textit{synapses}.

Back in 1943, Warren McCulloch and Walter Pitts published a seminal paper \cite{McCulloch1943}, in which they proposed the first mathematical model of an \textit{artificial neural network}, inspired by biological neural networks in animal brains. This model was named the McCulloch–Pitts neuron, or \textit{perceptron}. In some sense, it could ‘learn’ from given data and make decisions or predictions based on some parameters. The process of optimising the choice of such parameters is called \textit{training}.

Later in 1958, Frank Rosenblatt described an implementation of perceptron in detail \cite{Rosenblatt1958}. He is sometimes called the father of deep learning \cite{IEEE}. This field expanded quickly, and nowadays, neural networks are extensively used in various fields such as machine learning and artificial intelligence. We shall explore some simple applications later.

\subsection{Basic Concepts}

\subsubsection{Layers}

A neural network (abbreviated NN) consists of interconnected neurons, each of which can be thought as a mathematical function. They are typically organised into \textit{layers} consisting of

\begin{itemize}
    \item an \textit{input layer} that receives the data,
    \item multiple \textit{hidden layers} that processes the data, and
    \item an \textit{output layer} that produces the result.
\end{itemize}

The input and output data are usually given as real numbers. The whole neural network can therefore be thought as a function $f:\mathbb{R}^m\rightarrow\mathbb{R}^n$ that takes an input vector $\mathbf{x}$ of dimension $m$ and an output vector $\mathbf{y}$ of dimension $n$.

\subsubsection{Structure}

The ultimate purpose of training a neural network is to minimise the error between the predicted output $\mathbf{y}$ and targeted output $\mathbf{\hat{y}}$, usually given as a \textit{loss function} $L(\mathbf{y}, \mathbf{\hat{y}})$. Each connection between neurons has a \textit{weight}, and possibly a \textit{bias}, which are adjusted in the process of training, allowing the model to generalise to unseen data. They are usually given altogether as a \textit{weight matrix} $\mathbf{W}$ and a \textit{bias vector} $\mathbf{b}$.

\subsubsection{Activation Function}

For neural networks with multiple layers, an \textit{activation function} $\sigma$ is usually used. This is because it helps to decide whether a neuron should be ‘activated’ and introduces non-linearity into the model. The output of each neuron in a layer is therefore computed as $\mathbf{z}^l=\sigma\left(\mathbf{W}^l\mathbf{z}^{l-1}+\mathbf{b}^l\right)$, where $\sigma$ acts element-wise, $\mathbf{z}^l$ is the output vector for layer $l$, $\mathbf{W}^l$ is the weight matrix connecting layer $l-1$ to layer $l$, and $\mathbf{b}^l$ is the bias vector for layer $l$. 

As for the choice of the activation function, the S-shaped \textit{sigmoid function} has some nice properties: it is bounded, differentiable with a non-negative derivative, and has exactly one point of inflection. Two familiar examples of sigmoid functions are the \textit{logistic function} $\sigma(x)=1/\left(1+e^{-x}\right)$ and the hyperbolic tangent function $\sigma(x)=\tanh x$. Another commonly used activation function is the \textit{rectifier}, or \textit{rectified linear unit} (ReLU), defined as $\sigma(x)=\max(0, x)$.

\subsection{Example: Regression}
points to cover:
\begin{itemize}
    \item theory
    \item Lux.jl
    \item an implementation
\end{itemize}

\subsection{Example: Number Classification}
points to cover:
\begin{itemize}
    \item theory
    \item Flux.jl
    \item an implementation
\end{itemize}

\section{Introduction to Neural ODEs/CDEs}

\section{Solving Neural ODEs/CDEs}

\subsection{Solving Steps}

Solving differential equations has long been a difficult process, requiring much effort in existence theorems and techniques. Solving neural ODEs and NCDEs therefore poses a significant challenge in and of itself. Having defined basic neural network and neural ODEs/CDEs theory, we now turn our attention to the steps required to solve such problems numerically.

Consider \textcolor{red}{need to edit this based on the formal definition of the neural ODEs/CDEs we define, likely in an earlier section} a neural ODE defined by neural networks $f$ and $g$, such that $f_\theta$, when provided with parameters $\theta$, defines the differential equation of the system $\dot{x} = f_\theta\left(t, x\right)$, and $g_\phi$, when provided with paramters $\phi$, defines the initial value of the system $x\left(t_0\right) = g_\phi(t_0)$. In order to solve the neural ODE/CDE numerically, we are required to: 
\begin{enumerate}
    \item \textit{Train $g_\phi$ to predict the initial value of the system.} This requires the use of a loss function to train the neural network $g_\phi$, providing the parameters required to accurately predict the initial value of the system. 
    \item \textit{Train $f_\theta$ to provide the parameters of the neural ODE/CDE.} Similar to the prior step, this requires a loss function to train the neural network $f_\theta$, providing the parameters required to accurately state the specific ODE in question. Once the parameters have been approximated, we are left with a standard ODE problem.
    \item \textit{Solve the ODE Problem.} As most ODEs do not have analytical solutions, we make use of numerical ODE solvers to approximate the solution of the system.
\end{enumerate}

This section aims to define and discuss the methods and techniques required to conduct such numerical analysis on neural ODEs/CDEs. In particular, the basic ideas of loss functions, gradient descent and automatic differentiation are introduced here in Section 4.1. Sections 4.2 and 4.3 cover the methods used to train the relevant neural networks, and finally Section 4.4 discusses several numerical methods of solving the eventual ODEs after training. \textcolor{red}{edit section numbering if it changes. do this at the end when all has been confirmed}

\subsubsection{Loss Functions}
points to cover:
\begin{itemize}
    \item why we need loss functions
    \item definition of loss function
    \item how to choose loss functions
    \item basic examples
    \item what to do with loss functions?
\end{itemize}

\subsubsection{Gradient Descent and Applications}
points to cover:
\begin{itemize}
    \item motivation for gradient descent
    \item basic gradient descent and possible issues
    \item stochastic gradient descent - costly calculations
    \item adaptive gradient techniques - RMSProp, ADAM
    \item when to use what method for training
\end{itemize}

\subsubsection{Automatic Differentiation}
points to cover:
\begin{itemize}
    \item motivation behind AD
    \item starting with dual numbers
    \item extending dual numbers to higher dimensions
    \item extending dual numbers to multivariable contexts (taylor series)
    \item AD in general scenarios: chain rule
    \item julia packages implementing AD
\end{itemize}
\textcolor{red}{might be useful to move this to 3.2 instead, since the ideas in AD extend into backpropagation. but honestly it doesnt really matter since they're next to each other.}

\subsection{Backpropagation}

\subsection{log ODE Method}

\subsection{Numerical ODE Solvers}
Upon completion of training of the neural networks (where we have derived the learned parameters $\theta$, $\phi$), all that remains is to solve the subsequent ODE numerically. This can be done in a number of methods, which are generally known as the \textit{Runge-Kutta} methods.

\subsubsection{Euler's Method}
points to cover:
\begin{itemize}
    \item motivation for euler's method
    \item definition of euler's method
    \item proof of euler's method
    \item limitations of euler's method
    \item (maybe an implementation of euler's method? but its not going to be used anyway. see how)
\end{itemize}

\subsubsection{Runge-Kutta Methods}
points to cover:
\begin{itemize}
    \item general runge-kutta methods definition (including matrix representation)
    \item euler's method is a runge-kutta method (1st order)
    \item an example: RK4 (basic? proof and full extension of definition)
    \item actual runge-kutta methods used in julia packages - 5th orders and above  
    \item a short comment on what makes a good method for solving ODEs
\end{itemize}

\section{Application: TBC}

\section{Conclusion}
\textcolor{red}{The conclusion section is required but the previous sections (background, methods, results and discussion) are just examples of sections which may be useful.}


%%%%%%% TO BE DELETED AFTER %%%%%%%
\section{Examples (to be deleted)}

Here is a section with a few useful examples. 

\begin{figure}[b!] %Use h,t,b,! to enforce the location of the figure
    \centering
    \includegraphics[width=0.8\textwidth]{figures/imperial.jpg}
    \caption{This is an example of how you include a figure with a descriptive caption. This is an image of the South Kensington campus of Imperial College London on which we can recognize Queen's tower.}
    \label{fig:imperial-picture}
\end{figure}

Here is some filler text to show you what a few pages may look like. \lipsum[2-4] See Figure \ref{fig:imperial-picture}. \textbf{This is how you reference your figure in the text.}

\begin{table}[]
    \centering
    \begin{tabular}{llr}  
        \toprule
        \multicolumn{2}{c}{Module} \\
        \cmidrule(r){1-2}
        Module code    & Module name & Number of students \\
        \midrule
              & per gram    & 13.65      \\
                      &    each     & 0.01       \\
        Gnu       & stuffed     & 92.50      \\
        Emu       & stuffed     & 33.33      \\
        Armadillo & frozen      & 8.99       \\
        \bottomrule
    \end{tabular}
    \caption{Example booktabs table. Booktabs tables are nicer than regular ones. This site has a nice GUI for making LaTeX tables, and has a Booktabs option: https://www.tablesgenerator.com/}
    \label{tab:my_label}
\end{table}

\lipsum[1-4] \textbf{This is how you would reference a table:} Table \ref{tab:my_label}. 

\subsection{Section Example}
\label{sec:sec_example}
\lipsum[1]
\subsubsection{Subsection Example}
\label{sec:subsec_example}
\lipsum[1]

Note that you can reference chapters, sections, subsections and subsubsections. For example: Subsection \ref{sec:subsec_example}!

\subsection{Math Example}

While math can be written inline like so $f(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}$, we often need to write stand-alone equations like so
\begin{equation}
\textrm{score}(x) = \left(\lambda_m\sum_{i=0}^{|\mathbf{m}|} \log \hat{p}_m(d(x, \mathbf{m}_i) \mid l_i)\right) + \left(\lambda_l\sum_{i=0}^{|\mathbf{l}|} \log\hat{p}_l(d(x, \mathbf{l}_i) \mid \mathbf{v}_i)\right) + \lambda_p \hat{p}_p(x)
\end{equation}

To write equations over multiple lines (like systems of equation or equations too long to fit the page), one can use the \textit{align} environment coupled to the \textit{subequations} environment like so
\begin{subequations}
\begin{align}
    \mathbb{P}(0 \leadsto -a) &= D\int_0^\infty dt\, k_r e^{-k_r t}\int_0^t d\tau\,\partial_x u \big|_{x=-a}, \\
    \mathbb{P}(0 \leadsto b) &= -D\int_0^\infty dt\, k_r e^{-k_r t}\int_0^t d\tau\,\partial_x u \big|_{x=b}.
\end{align}
\end{subequations}

\subsection{Theorem and Proofs}

This section is directly taken from \href{https://www.overleaf.com/learn/latex/Theorems_and_proofs}{Overleaf}. Please do consult the webpage for more information and discuss with your supervisors. 

Theorems can easily be defined:

\begin{theorem}
Let \(f\) be a function whose derivative exists in every point, then \(f\) is 
a continuous function.
\end{theorem}

\begin{theorem}[Pythagorean theorem]
\label{pythagorean}
This is a theorem about right triangles and can be summarised in the next 
equation 
\[ x^2 + y^2 = z^2 \]
\end{theorem}

And a consequence of theorem \ref{pythagorean} is the statement in the next 
corollary.

\begin{corollary}
There's no right rectangle whose sides measure 3cm, 4cm, and 6cm.
\end{corollary}

You can reference theorems such as \ref{pythagorean} when a label is assigned.

\begin{lemma}
Given two line segments whose lengths are \(a\) and \(b\) respectively there is a 
real number \(r\) such that \(b=ra\).
\end{lemma}

As you can see, theorems, corollaries and lemmas are italicised. Often, definitions are not: 
\begin{definition}[Fibration]
A fibration is a mapping between two topological spaces that has the homotopy lifting property for every space \(X\).
\end{definition}
This can be changed using the \verb!\theoremstyle{}! command in the preamble. 

It can be useful to have an unnumbered theorem-like environment to add remarks, comments or examples to a mathematical document. Here, you can do so by using the following environment: 

\begin{remark}
This statement is true, I guess.
\end{remark}

Finally here is an example of proof: 

\begin{lemma}
Given two line segments whose lengths are \(a\) and \(b\) respectively there 
is a real number \(r\) such that \(b=ra\).
\end{lemma}

\begin{proof}
To prove it by contradiction try and assume that the statement is false,
proceed from there and at some point you will arrive to a contradiction.
\end{proof}


\subsection{Algorithm Example}

See Algorithm \ref{algorithm:posit}

\begin{algorithm}[]
\SetAlgoLined
\SetKwInOut{KwInput}{Input}
\SetKwInOut{KwOutput}{Output}
\SetKwInOut{KwPre}{Pre}
\SetKw{Return}{return}
\SetKwProg{Fn}{Function}{}{end}
\LinesNumbered
\KwInput{$\textbf{m}$, such that $\mathbf{m}_i$ is the position of the $i$'th monitor\newline
$\textbf{l}$, such that $\mathbf{l}_i$ is the position of the $i$'th landmark\newline
$\mathbf{p}^m$, such that $\mathbf{p}^m_i$ is the ping latency from monitor $i$ to the target\newline
$\mathbf{p}^l$, such that $\mathbf{p}^l_i$ is the set of ping latencies to landmark $i$}

\BlankLine
\KwPre{Compute $\hat{p}_m(d \mid l)$, an estimator giving the likelihood of the target being distance $d$ away from the monitor, given that the monitor records a latency of $l$ to that target. Implemented by training a KDE using $\mathbf{p}^l$.\newline
Compute $\hat{p}_l(d \mid v)$, an estimator giving the likelihood of the target being distance $d$ away from the landmark, given a Canberra distance of $v$ between the target and the landmark, using training targets.
}
\BlankLine
\KwOutput{Most likely location of the target}
\BlankLine

\Fn{Likelihood($x$, $\mathbf{v}$)} {
MonitorScore $\gets \sum_{i=0}^{|\mathbf{m}|} \log{\hat{p}_m(d(x, \mathbf{m}_i) \mid l_i)}$\;
LandmarkScore $\gets \sum_{i=0}^{|\mathbf{l}|} \log{\hat{p}_l(d(x, \mathbf{l}_i) \mid \mathbf{v}_i)}$\;
\Return MonitorScore + LandmarkScore
}

\BlankLine
$\mathbf{v} \gets $\{$\mathrm{canberra\_distance}(\mathbf{l}_i, \mathbf{p}^m) \mid \mathbf{l}_i \in \mathbf{l}$\}

$\mathbf{C}$ $\gets$ Constraint-Based-Geolocation($\mathbf{m}$, $\mathbf{p}^m$)\;
$\mathbf{C_l}$ $\gets$ \{$m \in \mathbf{m} \mid \mathbf{C}$ contains $m\} \cup \{l \in \mathbf{l} \mid \mathbf{C}$ contains $l$\}\;
\BlankLine
\Return argmax$_{x\in \mathbf{C_l}}$ Likelihood($x$)

 \caption{Algorithm example}
 \label{algorithm:posit}
\end{algorithm}

\subsection{Reference Example}

Here is how you can cite papers which you have added in the \verb!/bibs/bibliography.bib! file. You can cite single reference or multiple references.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
\addcontentsline{toc}{section}{Acknowledgement}
\textcolor{red}{Comment this out if not needed.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{First appendix}
\label{sec:appendix1}

\section{Second appendix}
\label{sec:appendix2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{bibliography.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}