{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a simple neural network that performs regression based on $y = \\sin x$, using the package Lux.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux, Random, Optimization, OptimizationOptimisers, ComponentArrays, Zygote, Plots, LinearAlgebra\n",
    "\n",
    "# Define data for regression\n",
    "n = 100\n",
    "x = range(-π, π; length = n)\n",
    "y = sin.(x)\n",
    "\n",
    "# Define a neural network model\n",
    "model = Chain(Dense(1 => 10, relu), Dense(10 => 10, relu), Dense(10 => 1))\n",
    "\n",
    "# Define the loss function based on 2-norm\n",
    "function regression_loss(ps, (model, st, (x, y)))\n",
    "    ŷ = vec(model(x', ps, st)[1])\n",
    "    return norm(ŷ - y)\n",
    "end\n",
    "\n",
    "# Train the neural network\n",
    "rng = MersenneTwister()\n",
    "ps, st = Lux.setup(rng, model)\n",
    "\n",
    "# Define the optimization problem\n",
    "prob = OptimizationProblem(OptimizationFunction(regression_loss, Optimization.AutoZygote()), ComponentArray(ps), (model, st, (x, y)))\n",
    "\n",
    "# Solve the optimization problem\n",
    "@time ret = solve(prob, Adam(0.03), maxiters = 250)\n",
    "\n",
    "# Plot the results\n",
    "plot(x, y, label=\"True\")\n",
    "plot!(x, vec(model(x', ret.u, st)[1]), label=\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider another simple neural network that does number recognition based on the MNIST dataset, using another package FLux.jl.\n",
    "\n",
    "[Reference: https://github.com/piotrek124-1/Simple_MNIST_Julia/tree/main]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, MLDatasets\n",
    "\n",
    "# Import the MNIST dataset\n",
    "x_train, y_train = MLDatasets.MNIST.traindata(Float32)\n",
    "x_test, y_test = MLDatasets.MNIST.testdata(Float32)\n",
    "\n",
    "# Use one-hot encoding for the labels\n",
    "y_train = Flux.onehotbatch(y_train, 0:9)\n",
    "\n",
    "# Define a neural network model\n",
    "model = Chain(Dense(784, 256, relu), Dense(256, 10, relu), softmax)\n",
    "\n",
    "# Define the loss function based on cross entropy\n",
    "loss(x, y) = Flux.Losses.logitcrossentropy(model(x), y)\n",
    "\n",
    "# Train the neural network\n",
    "parameters = Flux.params(model)\n",
    "train_data = [(Flux.flatten(x_train), Flux.flatten(y_train))]\n",
    "for i in 1:300\n",
    "    Flux.train!(loss, parameters, train_data, Adam(0.003))\n",
    "end\n",
    "\n",
    "# Test the neural network\n",
    "test_data = [(Flux.flatten(x_test), y_test)]\n",
    "accuracy = 0\n",
    "for i in 1:length(y_test)\n",
    "    if findmax(model(test_data[1][1][:, i]))[2] - 1  == y_test[i]\n",
    "        accuracy = accuracy + 1\n",
    "    end\n",
    "end\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy / length(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
