{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Neural Ordinary Differential Equations in Julia\n",
    "\n",
    "We'll deal with Neural ordinary differential equations(ODEs) problems when studying deep learning, which is a subset of maching learning. To let the computer learning automatically, we would use the algorithms utilize neural network architecture, inspired by biological neural networks found in human brain.\n",
    "\n",
    "In this part I will do a brief introduction to neural ODEs and show the implementation in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### =============================================\n",
    "### Layer Struct 构造单独的一层神经元\n",
    "### =============================================\n",
    "\n",
    "# A Layer consists of weights, bias and an activation function to produce the \n",
    "# output by its given input. And we'll save the result before applying the activation \n",
    "# function to compute the backward pass.\n",
    "\n",
    "struct Layer{WS, BS, Z, F}\n",
    "    # 设定参数来构造一层神经元\n",
    "    W::WS  # weights\n",
    "    b::BS  # bias\n",
    "    z::Z   # intermediate state 用来保存intermediate state的参数\n",
    "    σ::F   # activation function\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Layer constructor takes in the dimensions of input, output, and intermediate \n",
    "# state, and an activation function.\n",
    "\n",
    "Layer(in::Int, out::Int,  σ::Function) =    # Layer函数接收一个输入维度 输出维度 一个激活函数σ\n",
    "    # 返回一个包含初始化权重、偏置、中间状态向量和激活函数的Layer对象\n",
    "    Layer(rand(Float32, out, in) .- 0.5f0,  # weights 对这些参数进行初始化设置\n",
    "    zeros(Float32, out),                    # biases\n",
    "    Array{Float32}[],                       # intermediate state vector\n",
    "    σ)                                      # activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer output is computed by the formula σ(W * X + b)\n",
    "# 定义一个计算神经网络层输出的函数 函数使用给定的权重矩阵(W)、偏置(bias)、和激活函数来处理输入数据X\n",
    "function (l::Layer)(X)\n",
    "    W, b, z, σ = l.W, l.b, l.z, l.σ\n",
    "    temp = W * X .+ b\n",
    "    empty!(z)\n",
    "    push!(z, temp)   # store intermediate state for back propagation\n",
    "    return σ.(temp)  # apply the activation function element-wise\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function \"update!\" to update with partial derivatives and learning rate\n",
    "# 根据梯度和学习率(η)更新神经网络层的权重(W)和偏置(b)\n",
    "function update!((l::Layer, dW, db, η))\n",
    "    l.W .-= η * dW\n",
    "    l.b .-= η * db\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derive (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义了一个\"derive\"函数，用于计算神经网络层的权重、偏置和输入的梯度（偏导数）\n",
    "# 这些梯度用于反向传播算法中，以便调整网络的参数，从而最小化损失函数。\n",
    "function derive(l::Layer, ∂Cost∂a_out, a_in)\n",
    "    dσ = derive(l.σ) # 计算激活函数的导数\n",
    "\n",
    "    # 计算损失函数相对于中间结果\"z\"的导数\"∂Cost∂z\"\n",
    "    ∂Cost∂z = ∂Cost∂a_out .* dσ.(l.z[1])\n",
    "\n",
    "    # 计算整个批次的权重梯度\"∂Cost∂W\"\n",
    "    ∂W(∂Cost∂z, a_in) = ∂Cost∂z * a_in'\n",
    "    ∂Cost∂W = sum(∂W.(eachcol(∂Cost∂z), eachcol(a_in)))\n",
    "\n",
    "    # 计算偏置梯度\"∂Cost∂b\"\n",
    "    # 计算上一层输入的梯度\"∂Cost∂a_in\"\n",
    "    ∂Cost∂b = sum(eachcol(∂Cost∂z)) # Cost wrt input from last layer\n",
    "    ∂Cost∂a_in = l.W' * ∂Cost∂z\n",
    "    return ∂Cost∂W, ∂Cost∂b, ∂Cost∂a_in\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagate! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义\"back-propagate\"函数 \n",
    "# 用于在反向传播过程中更新神经网络层的权重和偏置 并计算损失函数相对于上一层输入的梯度\n",
    "function back_propagate!(l::Layer, ∂Cost∂a_out, a_in, η)\n",
    "    ∂Cost∂W, ∂Cost∂b, ∂Cost∂a_in = derive(l, ∂Cost∂a_out, a_in)  # gradients\n",
    "    \n",
    "    update!(l, ∂Cost∂W, ∂Cost∂b, η) # update parameters return ∂Cost∂a_in \n",
    "    return ∂Cost∂a_in               # Cost wrt input from last layer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Struct 构造模型\n",
    "# Model包含了多层神经元(Layers)\n",
    "# we'll store each layer's outputs in an array for the backward pass\n",
    "struct Model{LS, OS}\n",
    "    layers::LS  # Layers 多层神经元\n",
    "    a::OS       # Layer outputs\n",
    "\n",
    "    # Model Constructor\n",
    "    # 构造函数接受任意数量的层作为参数，并创建一个Model实例\n",
    "    Model(layers...) = new{typeof(layers), Vector{Array{Float32}}}(layers, [])\n",
    "\n",
    "    # Example 示例\n",
    "    # 假设我们有两个层 layer1 和 layer2，可以这样创建一个模型:\n",
    "    # layer1 = Layer(3, 5, relu)\n",
    "    # layer2 = Layer(5, 2, sigmoid)\n",
    "    # model = Model(layer1, layer2)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个用于评估Model对象的方法 通过按顺序评估每一层来处理输入数据X 并存储每层的输出\n",
    "# 通过逐层处理输入数据，并存储每层的输出，最终返回模型的输出结果\n",
    "# 有助于理解神经网络的前向传播过程，并为后续的反向传播提供必要的数据\n",
    "\n",
    "function (m::Model)(X)  # 类似于Model对象的call函数\n",
    "    # store model input\n",
    "    # 清空m.a并将数据X输入进去\n",
    "    empty!(m.a)\n",
    "    push!(m.a, X)\n",
    "\n",
    "    # evaluate each layer and store their outputs\n",
    "    for layer in m.layers\n",
    "        push!(m.a, layer(m.a[end]))\n",
    "    end\n",
    "\n",
    "    # 移除并返回数组 m.a 的最后一个元素 即模型的最终输出\n",
    "    return pop!(m.a)\n",
    "end\n",
    "\n",
    "# Example 示例\n",
    "# 假如有一个两层神经元的模型model\n",
    "# X = [1.0, 2.0, 3.0]\n",
    "# 那么在调用model(X)后 函数执行以下操作：\n",
    "# 1. 清空model.a 并将数据X存储在model.a中\n",
    "# 2. layer1处理数据X 计算输出 存储在model.a中\n",
    "#    layer2处理layer1的输出 计算输出 存储在model.a中\n",
    "# 3. 返回model.a的最后一个元素 即model2的输出 作为模型的最后输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "back_propagate! (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个用于对Model对象进行反向传播的方法 通过对每一层进行反向传播来更新模型的参数\n",
    "function back_propagate!(m::Model, ∂Cost∂aL, η) \n",
    "    # Back propagate through each layer ∂Cost∂a_out = ∂Cost∂aL\n",
    "    for layer in reverse(m.layers)\n",
    "        a_in = pop!(m.a)    # retrieve layer input\n",
    "        ∂Cost∂a_out = back_propagate!(layer, ∂Cost∂a_out, a_in, η) \n",
    "    end\n",
    "end\n",
    "\n",
    "# 这段代码实现了对神经网络模型的反向传播\n",
    "# 通过逐层反向传播计算梯度并更新参数 模型得以优化 损失函数逐渐减小\n",
    "# 每层的输入激活值和梯度依次传递 实现了完整的反向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义了一个训练神经网络模型的函数。\n",
    "# 通过在每个批次上进行前向传播、计算损失和反向传播来更新模型参数，函数逐步优化模型以最小化损失函数。\n",
    "function train!(m::Model, Cost, dataset, η)\n",
    "    # store cost of each batch in dataset\n",
    "    costs = Float32[]\n",
    "\n",
    "    # user-defined derivative function of Cost\n",
    "    dCost = derive(Cost)\n",
    "\n",
    "    # Train Model on each batch in dataset\n",
    "    for batch in dataset \n",
    "        X, Y = batch\n",
    "        out = m(X)\n",
    "\n",
    "        # Calculate cost\n",
    "        cost = Cost(out, Y)\n",
    "        push!(costs, cost)\n",
    "\n",
    "        # Back propagation\n",
    "        ∂Cost∂out = dCost(out, Y)\n",
    "        back_propagate!(m, ∂Cost∂out, η) \n",
    "    end\n",
    "\n",
    "    # Return average cost of all batches\n",
    "    return sum(costs) / length(dataset) \n",
    "end\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
